{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Equations\n",
    "This week we are going to start talking about how to solve differential equations.  For now we will focus on single first order differential equations.  Later, we will expand this to systems of equations and higher order equations.  A first order differential equation is an equation for an unknown function involving both the function and its first derivative.  By convention, we usually call the function $x(t)$ and we denot $\\frac{\\textrm{d}x}{\\textrm{d}t}$ by $\\dot{x}(t)$ instead of $x'(t)$.  (A dot above a function just means \"derivative with respect to time.\")  We will always assume that the equation can be solved for $\\dot{x}(t)$, and so we can write it in the form \n",
    "\n",
    "$\\dot{x}(t) = f(t, x(t)),\\hspace{1in}$ (1)\n",
    "\n",
    "where $f(t, x(t))$ is some formula involving $t$ and/or $x$.  In practice, the variable $x$ always appears in the formula on the right hand side, but often the variable $t$ does not appear explicitly.  If there is no $t$ in the formula, then we will sometimes write $\\dot{x}(t) = f(x)$ instead, but we will see later that python's builtin methods require us to write $f(t, x)$ even if we don't care about the $t$.  \n",
    "\n",
    "This is not a differential equations class, so we will not worry about methods for solving these equations exactly.  However, there are a few things worth noting.  First, any solution method necessarily requires taking an integral (to get rid of the derivative in $\\dot{x}(t)$), so we will always get a constant of integration in our solution.  This means that the solution to equation (1) is a function $x(t)$ with an arbitrary constant in it.  To determine this constant, we need one more piece of information.  This information is essentially always given in the form $x(t_0) = x_0$, where $t_0$ and $x_0$ are known constants.  This is called an *initial condition* or *initial value* for $x$.  This means that we will always know the value of $x$ at one particular time.  Technically, $t_0$ can be any number, but we will always just assume that it is $0$.  (If you are given a different $t_0$, it is easy to rewrite $f(t, x)$ so that you can use $t_0$ instead.)  Our equations will therefore always be of the form \n",
    "\n",
    "$\\dot{x}(t) = f(t, x)$ and $x(0) = x_0$.  \n",
    "\n",
    "When you are given both the differential equation and the initial condition, we call it an *initial value problem* (IVP).  \n",
    "\n",
    "The following theorem about initial value problems is very useful: As long as $f$ is reasonably nice (not a piecewise function, for instance, but continuously differentiable is good enough) then it has a unique solution.  This means that we never have to worry about not having a solution to find or having more than one possible answer.  \n",
    "\n",
    "While it can be quite difficult to find the solution to an initial value problem, it is easy to check if a given function is a solution.  If someone gives you a function $x(t)$ and an initial value problem, you can just plug in 0 and check that $x(0) = x_0$, then take the derivative of $x$ and plug it into the formula to check that $\\dot{x} = f(t, x)$.  For example, it is easy to check that the solution to the initial value problem \n",
    "\n",
    "$\\dot{x}(t) = \\lambda x$ and $x(0) = x_0$\n",
    "\n",
    "is given by \n",
    "\n",
    "$x(t) = x_0e^{\\lambda t}$\n",
    "\n",
    "(Here, $\\lambda$ is just a constant.)  Likewise, it is easy to check (if you remember some trig identities) that the solution to \n",
    "\n",
    "$\\dot{x}(t) = x^2 + 1$ and $x(0) = 0$\n",
    "\n",
    "is given by \n",
    "\n",
    "$x(t) = \\tan(t)$\n",
    "\n",
    "Unfortunately, even though every initial value problem has a unique solution, it is usually impossible to even write down the formula for that solution, let alone calculate it explicitly.  We can therefore never hope to find the actual formula for $x(t)$.  The best we can do is numerically approximate this function by finding $x$ values at lots of different $t$ values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Solutions\n",
    "With this in mind, we will choose a list of $t$ values $t_0$, $t_1$, $t_2$, $\\dotsc$, $t_k$, $\\dotsc$, $t_N$.  To make life easy, we will always start at $t_0 = 0$ and choose our $t$ values to be evenly spaced.  That is, we will always make sure that $t_{k+1} - t_k = \\Delta t$ is the same for all $k$.  This means that we have the formula $t_k = k\\Delta t$.  We will also usually name the final time $t_N = T$.  \n",
    "\n",
    "In an ideal world, we would find the value of $x$ at each of these times.  That is, we would find $x(t_0)$, $x(t_1)$, $x(t_2)$, $\\dotsc$, $x(t_k)$, $\\dotsc$, $x(t_N)$.  However, we can't actually find the formula for $x(t)$, so we generally can't find these values exactly.  Instead, we will try to find approximations to each of these $x$ values.  We will call the approximations $x_0$, $x_1$, $x_2$, $\\dotsc$, $x_k$, $\\dotsc$, $x_N$.  We want $x_k \\approx x(t_k)$.  Notice that $x_0 = x(t_0)$ is not actually an approximation; it is the initial condition that we are already given, and so it is exact.  Every other $x_k$, however, will only be approximately correct.  \n",
    "\n",
    "For the moment, let's ignore the details of how we might find these approximate $x$ values and just suppose that (for any given $\\Delta t$ and $T$) we can produce a list of $x_k$'s.  We would like to know how good our approximation is and how the quality of our approximation depends on $\\Delta t$ (and perhaps how it depends on $T$ or $f$ or $x_0$ as well).  There are quite a few different ways we could phrase this question, but we will focus on three popular versions: \n",
    "\n",
    "1) How much error do you accrue between two consecutive times?  That is, if $x_k$ is off by some amount from $x(t_k)$, then how much more (or possibly less) off will $x_{k+1}$ be from $x(t_{k+1})$?  It turns out that, for a fairly wide array of initial value problems, this doesn't really depend on $k$, and so we can just phrase it in terms of the first approximation: How big is the error at $t_1$?  That is, how big is $x_1 - x(t_1)$?  It should be obvious from the definitions that this depends on $\\Delta t$ but not on $T$.  We will call this error the *local accuracy*.  \n",
    "\n",
    "2) How much error do you accrue by the final time $T$?  That is, how big is $x_N - x(t_N)$?  It turns out that this depends on both $\\Delta t$ and $T$ in complicated ways, but for all of the methods that we will look at, you can write this error as $\\mathcal{O}(\\Delta t^p)$, where $p$ is some power.  This means that if you hold $T$ constant, then the error is proportional to some power of $\\Delta t$, and the power does not depend on $T$.  We call this error the *global accuracy*.  \n",
    "\n",
    "3) How much error do you accrue if you let the process run forever?  That is, what happens to the global accuracy when you let $T\\to\\infty$?  This turns out to be a much more delicate question.  The general concept is known as *stability*, but mathematicians have studied many different versions of stability.  We will look at one of the simplest versions in the next lecture.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Euler Method (first derivation)\n",
    "Now let's derive a method for finding the approximations $x_k$.  We have the equation $\\dot{x}(t) = f(t, x)$.  In general, we will already know the formula for $f$, but even if we knew $x(t_k)$ exactly for all of our times, this equation wouldn't help us because we still wouldn't know $\\dot{x}(t)$.  To get around this problem, we can try to approximate $\\dot{x}(t)$ using some of our $x$ values.  For example, let's use the first order forward difference scheme that we derived last week to approximate $\\dot{x}(t)$.  That is, we will write \n",
    "\n",
    "$\\dot{x}(t) \\approx \\frac{x(t + \\Delta t) - x(t)}{\\Delta t}$.\n",
    "\n",
    "This approximation is valid for any time, so in particular it is valid at time $t = t_0$.  We therefore have \n",
    "\n",
    "$\\dot{x}(t_0) \\approx \\frac{x(t_0 + \\Delta t) - x(t_0)}{\\Delta t} = \\frac{x(t_1) - x_0}{\\Delta t}$.  \n",
    "\n",
    "We can therefore (approximately) rewrite our differential equation as \n",
    "\n",
    "$\\frac{x(t_1) - x_0}{\\Delta t} \\approx f(t_0, x(t_0)) = f(t_0, x_0)$.  \n",
    "\n",
    "If we rearrange this, we get \n",
    "\n",
    "$x(t_1) \\approx x_0 + \\Delta t f(t_0, x_0)$.  \n",
    "\n",
    "Of course, this isn't the actual $x(t_1)$, but if $\\Delta t$ is small enough then it should be a good approximation, so we will take this formula as our definition for $x_1$.  That is, \n",
    "\n",
    "$x_1 = x_0 + \\Delta t f(t_0, x_0)$.  \n",
    "\n",
    "Notice that we know everything on the right hand side of this equation.  We are given $x_0$, $t_0$, $f$ and $\\Delta t$ at the start of the problem, so this formula gives us $x_1$.  We say that this is an *explicit* formula because it is already solved for $x_1$ in terms of things we already know.  \n",
    "\n",
    "We can do exactly the same thing at time $t = t_1$.  Our difference scheme is then \n",
    "\n",
    "$\\dot{x}(t_1) \\approx \\frac{x(t_1 + \\Delta t) - x(t_1)}{\\Delta t} = \\frac{x(t_2) - x(t_1)}{\\Delta t}$.  \n",
    "\n",
    "Plugging this into our differential equation at time $t_1$ and rearranging, we get \n",
    "\n",
    "$x(t_2) \\approx x(t_1) + \\Delta t f(t_1, x(t_1))$.  \n",
    "\n",
    "We don't know $x(t_1)$ or $x(t_2)$, but we do already have an approximation to $x(t_1)$, so we have \n",
    "\n",
    "$x(t_2) \\approx x_1 + \\Delta t f(t_1, x_1)$.  \n",
    "\n",
    "As before, we will just take this as the definition for $x_2$, so we have \n",
    "\n",
    "$x_2 = x_1 + \\Delta t f(t_1, x_1)$.  \n",
    "\n",
    "Once again, this is an explicit equation for $x_2$.  We just found $x_1$ in the last step, and everything else in this formula was given at the beginning of the problem.  \n",
    "\n",
    "We can keep repeating this process for as long as we want.  In general, if we have already found all the $x$ values up to $x_k$, then we get the formula \n",
    "\n",
    "$x_{k+1} = x_k + \\Delta t f(t_k, x_k)$.  \n",
    "\n",
    "This method of approximation is called the *forward Euler method*, because we used a forward difference scheme to approximate $\\dot{x}$.  We say that the forward Euler method is a *time stepping method* because we find the approximation to $x$ at each time in turn.  That is, we find an approximation for $x(t_1)$, then an approximation for $x(t_2)$, then an approximation for $x(t_3)$, etc., and we never go back and find a new approximation for an earlier time.  The forward Euler method is also called *explicit* because at each step we have to solve an explicit equation for the next approximation.  \n",
    "\n",
    "Unfortunately, it is somewhat difficult to determine the local and global accuracy of the forward Euler method from these formulas.  (The local error is not so bad, but the global error will be difficult because we have no idea how the errors from the difference scheme approximations compound as we step through time.)  To determine the accuracy, it will be more convenient to re-derive the forward Euler method in a slightly different way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Euler Method (second derivation)\n",
    "Remember that we are trying to solve the differential equation \n",
    "\n",
    "$\\frac{\\textrm{d}x}{\\textrm{d}t} = f(t, x(t))$.  \n",
    "\n",
    "Instead of approximating the derivative here, let's try to get rid of it by integrating both sides.  Here, we will exploit the fundamental theorem of calculus: \n",
    "\n",
    "$\\displaystyle\\int_{a}^{b}\\frac{\\textrm{d}x}{\\textrm{d}t}\\,\\textrm{d}t = x(b) - x(a)$.  \n",
    "\n",
    "In particular, let's integrate both sides of our differential equation from $t_k$ to $t_{k+1}$.  We get \n",
    "\n",
    "$\\displaystyle\\int_{t_k}^{t_{k+1}}\\frac{\\textrm{d}x}{\\textrm{d}t}\\,\\textrm{d}t = \\displaystyle\\int_{t_k}^{t_{k+1}}f(t, x(t))\\,\\textrm{d}t$, \n",
    "\n",
    "and so \n",
    "\n",
    "$x(t_{k+1}) - x(t_k) = \\displaystyle\\int_{t_k}^{t_{k+1}}f(t, x(t))\\,\\textrm{d}t$.  \n",
    "\n",
    "Of course, there's no such thing as a free lunch in mathematics.  We still don't know what $x(t)$ is, and so we can't actually integrate the right hand side.  However, we can try to approximate this integral using one of the methods from last week.  In particular, we will use the left hand rule.  Since $t_k$ and $t_{k+1}$ are only $\\Delta t$ apart, it makes sense to approximate this integral with only one rectangle.  We therefore have \n",
    "\n",
    "$x(t_{k+1}) - x(t_k) \\approx \\Delta t f(t_k, x(t_k))$, \n",
    "\n",
    "and so \n",
    "\n",
    "$x(t_{k+1}) \\approx x(t_k) + \\Delta t f(t_k, x(t_k))$.  \n",
    "\n",
    "We don't know either $x(t_k)$ or $x(t_{k+1})$, but we can just use this formula for our approximations, so we have \n",
    "\n",
    "$x_{k+1} = x_k + \\Delta t f(t_k, x_k)$.  \n",
    "\n",
    "This is exactly the same as the formula we found in the last section, and so we have just re-derived the forward Euler method.  The nice thing about this derivation is that we know how the error from the left hand rule accumulates as we integrate over longer and longer time intervals.  In particular, we know that the left hand rule has second order local error and first order global error.  The local error of the left hand rule translates directly into local accuracy for the forward Euler method, so we say that the forward Euler method has a local accuracy of $\\mathcal{O}(\\Delta t^2)$.  Furthermore, the global error of the left hand rule also translates into global accuracy for the forward Euler method.  (The proof for this part is a little trickier, but we won't worry about the details here.)  We therefore say that the forward Euler method has a global accuracy of $\\mathcal{O}(\\Delta t)$.  We usually care more about the global accuracy than the local accuracy, so we say that the forward Euler method is first order accurate.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
