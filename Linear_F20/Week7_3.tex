\documentclass[reqno]{amsart}


\pagestyle{empty}

\usepackage{graphicx}
\usepackage[margin = 1cm]{geometry}
\usepackage{color}
\usepackage{cancel}
\usepackage{multirow}
\usepackage{framed}
\usepackage{amssymb}
\usepackage{stackengine}
\usepackage{tikz}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newenvironment{handwave}{%
  \renewcommand{\proofname}{Handwavey proof}\proof}{\endproof}
  %\renewcommand{\qedsymbol}{$\blacksquare$}

\begin{document}
\begin{flushleft}
{\sc \Large AMATH 352 Rahman} \hfill Week 7
\bigskip
\end{flushleft}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\CancelColor}{\color{red}}
\newcommand{\?}{\stackrel{?}{=}}
\renewcommand{\varphi}{\phi}
\newcommand{\card}{\text{Card}}
\newcommand{\bigzero}{\text{\Huge 0}}
\newcommand{\curvearrowdown}{{\color{red}\rotatebox{90}{$\curvearrowleft$}}}
\newcommand{\curvearrowup}{{\color{red}\rotatebox{90}{$\curvearrowright$}}}

\newcommand*\circled[1]{\color{red}\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}



\section*{Sec. 5.4 Least squares}

We noticed that we often come across matrices with no solution.  In class we have ignored them for the
most part, but in real life we can't.  One way to ``solve'' this is to throw out data points, but this results
in large errors.  Lets think of a way to minimize this average error instead of throwing out data points.

We think of a squared error:  Suppose we have data points $2x_1 = b_1$, $3x_2 = b_2$, $4x_3 = b_3$.
One way to minimize the error between $x$ and the data points is to let
%
\begin{equation*}
E^2 = (2x - b_1)^2 + (3x - b_2)^2 + (4x - b_3)^2.
\end{equation*}
%
If there was an exact solution then $E^2 = 0$.  In the more likely case $E^2$ will be a parabola.  We can find
the minimum of a parabola just by taking the derivative and finding critical points, then doing the max and min
test.
%
\begin{equation*}
\frac{dE^2}{dx^2} = 2[2(2x - b_1) + 3(3x - b_2) + 4(4x - b_3)] = 0.
\end{equation*}
%
Solving for $x$ gives us
%
\begin{equation*}
x = \frac{2b_1 + 3b_2 + 4b_3}{2^2 + 3^2 + 4^2} = \frac{a^Tb}{a^Ta},
\end{equation*}
%
which is called the least square ``solution'' in one variable.  In order to separate the notation, we shall call
the least squares solution $\hat{x}$ instead of $x$.

For multiple variables, all vectors perpendicular to the column space lie on the left nullspace.  Thus the error
vector $e = b - A\hat{x}$ must be in the nullspace of $A^T$:  $A^T(b - A\hat{x}) = 0 \Rightarrow A^TA\hat{x} = A^Tb$.
Then $\hat{x} = (A^TA)^{-1}(A^Tb)$.  And the projection itself would be $p = A\hat{x} = A(A^TA)^{-1}(A^Tb)$.

\begin{itemize}

\item[Ex:  ]  Consider
%
\begin{equation*}
A = \begin{pmatrix}
1 & 2\\
1 & 3\\
0 & 0
\end{pmatrix},\qquad b = \begin{pmatrix}
4\\
5\\
6
\end{pmatrix}
\end{equation*}
%

Notice that $Ax = b$ has no solution.  So we try to find a least squares solution by using the \underline{normal equation}:
$A^TA\hat{x} = A^Tb$.
%
\begin{equation*}
A^TA = \begin{pmatrix}
1 & 1 & 0\\
2 & 3 & 0
\end{pmatrix} \begin{pmatrix}
1 & 2\\
1 & 3\\
0 & 0
\end{pmatrix} = \begin{pmatrix}
2 & 5\\
5 & 13
\end{pmatrix}
\end{equation*}
%
and
%
\begin{equation*}
A^Tb = \begin{pmatrix}
1 & 1 & 0\\
2 & 3 & 0
\end{pmatrix}\begin{pmatrix}
4\\
5\\
6
\end{pmatrix} = \begin{pmatrix}
9\\
23
\end{pmatrix}
\end{equation*}
%
We can either solve this through Gaussian elimination or by inverting the matrix.  For this particular problem
we chose to invert.
%
\begin{equation*}
\hat{x} = (A^TA)^{-1}(A^Tb) = \begin{pmatrix}
13 & -5\\
-5 & 2
\end{pmatrix}\begin{pmatrix}
9\\
23
\end{pmatrix} = 
\begin{pmatrix}
2\\
1
\end{pmatrix}
\end{equation*}
%
Then the projection is
%
\begin{equation*}
p = A\hat{x} = \begin{pmatrix}
1 & 2\\
1 & 3\\
0 & 0
\end{pmatrix} \begin{pmatrix}
2\\
1
\end{pmatrix} = \begin{pmatrix}
4\\
5\\
0
\end{pmatrix}
\end{equation*}

\pagebreak

\item[Ex:  ]  Consider
%
\begin{equation*}
A = \begin{pmatrix}
2 & 1\\
1 & 2\\
1 & 1
\end{pmatrix},\qquad b = \begin{pmatrix}
2\\
0\\
-3
\end{pmatrix}
\end{equation*}

First we find the normal equation $A^TAx = A^Tb$,
%
\begin{equation*}
A^TA = \begin{pmatrix}
2 & 1 & 1\\
1 & 2 & 1
\end{pmatrix} \begin{pmatrix}
2 & 1\\
1 & 2\\
1 & 1
\end{pmatrix} = \begin{pmatrix}
6 & 5\\
5 & 6
\end{pmatrix}
\end{equation*}
%
And
%
\begin{equation*}
A^Tb = \begin{pmatrix}
2 & 1 & 1\\
1 & 2 & 1
\end{pmatrix} \begin{pmatrix}
2\\
0\\
-3
\end{pmatrix} = \begin{pmatrix}
1\\
-1
\end{pmatrix}
\end{equation*}
%
We can see the solution without even inverting or doing Gaussian elimination: $\hat{x} = (1, -1)$.
Notice that gives us the line $y = -x$.

Also, the projection is
%
\begin{equation*}
p = A\hat{x} = \begin{pmatrix}
2 & 1\\
1 & 2\\
1 & 1
\end{pmatrix}\begin{pmatrix}
1\\
-1
\end{pmatrix} = \begin{pmatrix}
1\\
-1\\
0
\end{pmatrix}
\end{equation*}

\item[Ex:  ]  Consider
%
\begin{equation*}
A = \begin{pmatrix}
1 & 0 & 1\\
1& 1 & 1\\
0 & 1 & 1\\
1 & 1 & 0
\end{pmatrix},\qquad b = \begin{pmatrix}
4\\
-1\\
0\\
1
\end{pmatrix}
\end{equation*}

First we find the normal equation
%
\begin{equation*}
A^TA = \begin{pmatrix}
1 & 1 & 0 & 1\\
0 & 1 & 1 & 1\\
1 & 1 & 1 & 0
\end{pmatrix} \begin{pmatrix}
1 & 0 & 1\\
1 & 1 & 1\\
0 & 1 & 1\\
1 & 1 & 0
\end{pmatrix} = \begin{pmatrix}
3 & 2 & 2\\
2 & 3 & 2\\
2 & 2 & 3
\end{pmatrix}
\end{equation*}
%
and
%
\begin{equation*}
A^Tb = \begin{pmatrix}
1 & 1 & 0 & 1\\
0 & 1 & 1 & 1\\
1 & 1 & 1 & 0
\end{pmatrix} \begin{pmatrix}
4\\
-1\\
0\\
1
\end{pmatrix} = \begin{pmatrix}
4\\
0\\
3
\end{pmatrix}
\end{equation*}
%
Then we solve the normal equation
%
\begin{equation*}
\begin{pmatrix}
3 & 2 & 2 & | & 4\\
2 & 3 & 2 & | & 0\\
2 & 2 & 3 & | & 3
\end{pmatrix} = \begin{pmatrix}
6 & 4 & 4 & | & 8\\
6 & 9 & 6 & | & 0\\
6 & 6 & 9 & | & 9
\end{pmatrix} = \begin{pmatrix}
6 & 4 & 4 & | & 8\\
0 & 5 & 2 & | & -8\\
0 & 2 & 5 & | & 1
\end{pmatrix} = \begin{pmatrix}
6 & 4 & 4 & | & 8\\
0 & 10 & 25 & | & 5\\
0 & 10 & 4 & | & -16
\end{pmatrix} = \begin{pmatrix}
6 & 4 & 4 & | & 8\\
0 & 10 & 25 & | & 5\\
0 & 0 & -21 & | & -21
\end{pmatrix}.
\end{equation*}
%
Then our least square solution for $\hat{x} = (2, -2, 1)$, and the projection is
%
\begin{equation*}
p = A\hat{x} = \begin{pmatrix}
1 & 0 & 1\\
1 & 1 & 1\\
0 & 1 & 1\\
1 & 1 & 0
\end{pmatrix} \begin{pmatrix}
2\\
-2\\
1
\end{pmatrix} = \begin{pmatrix}
3\\
1\\
-1\\
0
\end{pmatrix}
\end{equation*}

\pagebreak

\item[Ex:  ]  What is the linear regression of the points $(-1,1),\, (1,0),\, (3,-3)$?


Here we are given data points and we need to find the least square fit (also called regression).
Lets first write this as a system of equations with $y = mx + b$,
%
\begin{align*}
b + -m &= 1\\
b + m &= 0\\
b + 3m &= -3
\end{align*}
%
Then the matrix form of this is
%
\begin{equation*}
\begin{bmatrix}
1 & -1\\
1 & 1\\
1 & 3
\end{bmatrix} \begin{bmatrix}
b\\
m
\end{bmatrix} = \begin{bmatrix}
1\\
0\\
-3
\end{bmatrix}
\end{equation*}
%
Then we find the normal equation
%
\begin{equation*}
A^TA = \begin{bmatrix}
1 & 1 & 1\\
-1 & 1 & 3
\end{bmatrix} \begin{bmatrix}
1 & -1\\
1 & 1\\
1 & 3
\end{bmatrix} = \begin{bmatrix}
3 & 3\\
3 & 11
\end{bmatrix}
\end{equation*}
%
and
%
\begin{equation*}
A^Tb = \begin{bmatrix}
1 & 1 & 1\\
-1 & 1 & 3
\end{bmatrix} \begin{bmatrix}
1\\
0\\
-3
\end{bmatrix} = \begin{bmatrix}
-2\\
-10
\end{bmatrix}
\end{equation*}
%
Then we have
%
\begin{equation*}
(A^TA)\begin{pmatrix}
b\\
m
\end{pmatrix} = A^Tb \Rightarrow b = 1/3, m = -1 \Rightarrow \hat{y} = -\hat{x} + 1/3.
\end{equation*}

\item[Ex:  ]  What is the linear regression for $(-2,1),\, (-1,2),\, (0,1),\, (1,2),\, (2,1)$.


Again, we fill in the matrix for least squares
%
\begin{equation*}
A = \begin{bmatrix}
1 & -2\\
1 & -1\\
1 & 0\\
1 & 1\\
1 & 2
\end{bmatrix}
\end{equation*}
%
Then our normal equation becomes
%
\begin{equation*}
A^TA = \begin{bmatrix}
5 & 0\\
0 & 10
\end{bmatrix},\qquad \text{  and  } \qquad A^Tb = \begin{bmatrix}
7\\
0
\end{bmatrix}
\end{equation*}
%
So, $b = 7/5$ and $m = 0$, then $\hat{y} = 7/5$, which is a horizontal line.

\end{itemize}

For a quadratic fit we simply fill in the quadratic equation $y = ax^2 + bx + c$ (I'll give you this on the HW).  What would we do for an exponential fit?



\end{document}